%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2022}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2022}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2022}

\begin{document}

\twocolumn[
\icmltitle{Opti-ML: An End-to-End Architecture for \\
ML Guided Inlining for Performance}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2022
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Sam Keyser}{MSOE}
\icmlauthor{Jonny Keane}{MSOE}
\icmlauthor{Salvin Chowdhury}{MSOE}
\icmlauthor{Jeremy Kedziora}{MSOE}
\end{icmlauthorlist}

\icmlaffiliation{MSOE}{Electrical Engineering \& Computer Science, Milwaukee School of Engineering, Milwaukee, WI, United States}

\icmlcorrespondingauthor{Sam Keyser}{keysers@msoe.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Reinforcement Learning, Compilers}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
FOO BAR BAZ
\end{abstract}

\section{Introduction}
\label{introduction}
\begin{itemize}
\item Introduce the problem\@.
\item Introduce contribution\@.
\item Introduce structure of the rest of the paper\@.
\end{itemize}

LLVM is a modern framework for creating compilers. It decouples the parsing of source code from the generation of valid machine code by introducing an intermediate representation (IR) that captures program semantics in a language agnostic format \cite{llvm-paper}. LLVM is also an optimizing compiler. It performs several passes on program IR in an attempt to improve program size, runtime, or both. A "pass" is a single traversal through either the complete code, or a segment. These optimizations can be performed at several granularities such as Function, Call Graph, Loop, or Module level \cite{lattner2008}. Passes can generally either gather information or perform a transformation on the module of code \cite{llvm-pass-docs}. Some passes may also be run several times, either in sequence or after other passes have been performed to take advantage of new changes introduced by prior passes. Passes have been tenatively organized into pipelines such as -O\{1, 2, 3, s, z\} by compiler authors which optimize for either runtime speed or binary size, at the expense of increased compile time. These pipelines can be quite involved; LLVM's -O3 pass involves approximately 160 optimizations passes which involves optimizations performed at multiple different levels of granularity.

% TODO: should I include a vocabulary section?

Function inlining is one of the fundamental optimizations implemented by modern compilers. Function inlining passes are applied at the function level and examine each call site within the function. The decision is then made to either perform inlining or not. Inlining is the replacement of the callsite with the entire function definition. It can be thought of as "copy-pasting" the function definition wherever the function would have been called. Function inlining can improve runtime performance by removing the overhead of jumping to the caller body from the callsite. Additionally, it can improve the performance of downstream optimization passes such as escape analysis by adding more information the current function context \cite{Theodoridis_Grosser_Su_2022}.

Function inlining can have an impact on both the final binary size and the runtime of the final binary. Inlining small functions or functions which are called very infrequently is usually innocuous. The benefit derived from not having to jump to the function definition outweighs the increase in size from duplicating the function definition at each callsite. It becomes more fuzzy when functions are both large and called frequently. Inlining such functions would likely improve the runtime, but would also increase the final binary size. The answer in such a case is not clear. In fact, the inlining problem can be shown as equivalent to a 0-1 knapsack problem, making it NP-complete \cite{Theodoridis_Grosser_Su_2022}. 

% TODO: need to find a way to introduce the idea of "inlining for size" and "inlining for sped"
Function inlining is traditionally solved by heuristics developed by compiler designers. Designing good heuristics is a non-trivial task, but recent work has shown promise in machine-learned policies as an alternative to manually defined policies \cite{mlgo}. To our knowledge, two prior works have approached function inlining using machine learning methods: MLGO \cite{mlgo} and MLGOPerf \cite{mlgoperf}. MLGO chose to focus on creating a heuristic for inlining for size due to several challenges with training an inlining for speed heuristic. MLGOPerf built on the work done by Trofin et al. and proposed a framework for training inlining for speed agents.

%TODO: I feel like I should describe on MLGOPerf so that the reader has context for this contribution section
Opti-ML builds on top of the work of Ashouri et al. with the following additions\footnote{While not a novel addition, we also produced an open source implementation of MLGOPerf's training architecture: \href{https://github.com/TrainingAfternoon/opti-ml}{https://github.com/TrainingAfternoon/opti-ml}}:
\begin{enumerate}
    % TODO: this should go somewhere else
    %\item An open source implementation of the MLGOPerf training infrastructure\footnote{https://github.com/TrainingAfternoon/opti-ml}\footnote{This repository is still actively being cleaned up as of the time you are receiving this rough draft. The code will be significantly more documented by the end of week 16}
    \item The collection of a dataset based on CppPerfBenchmarks \cite{cpp-perf-benchmarks} for training inlining for speed agents
    \item The replacement of the feature extraction layer within MLGOPerf's IR2Perf model with an embedding layer, IR2Vec \cite{ir2vec}. %MLGOPerf relies on extracting a set of manually chosen features from the compiler, which are fed to their model. IR2Vec introduces the ability to embed the entire representation of a module of code
    \item The use of random forest as an alternative to deep neural networks for IR2Perf
\end{enumerate}

The rest of the paper is organized as follows. Section 2 discusses the architectures of MLGO and MLGOPerf in more detail, as well as IR2Vec. Section 3 provides detail on how we collected our dataset and our proposed inlining-for-speed training scheme as well as the hardware \& sofwater used for training our agent. Section 4 presents the results of our function inlining heuristic against the default inlining heurstic in LLVM with a varied set of parameters. We also compare against the agents trained by MLGO and MLGOPerf respectively. Finally, section 5 discusses current shortcomings, challenges, and proposes future work.

%Opti-ML builds upon these efforts by creating a full implementation of the MGLOPerf inlining-for-speed training infrastructure, as well as a dataset based on CppPerformanceBenchmarks \cite{cpp-perf-benchmark}. Finally, Opti-ML also replaces the feature extraction layer within MLGOPerf's IR2Perf network with an embedding layer, IR2Vec \cite{ir2vec}.

%Opti-ML proposes the following 
%\begin{enumerate}
%    \item The replacement of the feature extraction layer within MLGOPerf's IR2Perf model with an embedding layer, IR2Vec \cite{ir2vec}. MLGOPerf relies on the 
%\end{enumerate}

%This work builds upon previous efforts to develop a policy for the function inlining optimization pass in the LLVM C/C++ compiler  \cite{mlgo, mlgoperf}.

%These optimizations can be performed at several granularities such as Function, Call Graph, Loop, or Module level \cite{lattner2008}. 
%Optimizing compilers use heuristics to guide optimizations such as register allocation and function inlining. These heuristics are frequently hand crafted by experts \cite{TODO} and are sometimes augmented with compile-time feedback from Profile Guided Optimization (PGO)  \cite{PGO}.
%Designing good heuristics is a non-trivial task  \cite{TODO}, but recent work has shown promise in machine-learned policies as an alternative to manually defined policies  \cite{mlgo}. This work builds upon previous efforts to develop a policy for the function inlining optimization pass in the LLVM C/C++ compiler  \cite{mlgo}.

\section{Background}
\label{background}
\begin{itemize}
\item MLGO\@.
\item MLGOPerf\@.
\item IR2Vec\@.
\end{itemize}

\subsection{MLGO}
MLGO, or "Machine Learning Guided Compiler Optimization Framework" \cite{mlgo}, was an attempt to introduce machine learning guided compiler optimizations into a "real world" compiler environment. The authors chose to target the LLVM C/C++ compiler and focused on two compiler optimizations: function inlining and register allocation.


%MLGO worked on function inlining as an incidental problem to their efforts to add the framework for using reinforcement learning (RL) to produce compiler heuristics in lieu of a human. They created a heuristic which used function inlining to specifically optimize for final binary size, rather than binary runtime. MLGOPerf built on top of the framework introduced by Trofin et al. in order to train a function inlining agent to optimize for binary runtime.

\subsection{MLGOPerf}

\subsection{IR2Vec}

\section{Methods}
\label{methods}
\begin{itemize}
\item Hardware / software report\@.
\item Data sources\@.
\item Data collection\@.
\item IR2Perf Training\@.
\item MLGO Training\@.
\end{itemize}

\section{Results}
\label{results}
\begin{itemize}
\item IR2Perf Training results + data EDA\@.
\item IR2Vec-MLGOPerf / MLGOPerf / MLGO / base (with many diff. heuristic values) results on cpp-perf-benchmark, cbench\@.
\end{itemize}

\section{Discussion}
\label{discussion}
\begin{itemize}
\item Impact of IR2Vec on MLGOPerf on CBench\@.
\item Comparison of varying in-lining heuristic with MLGOPerf performance\@.
\end{itemize}

\section{Conclusion}
\label{conclusion}
\begin{itemize}
\item Summarize takeaways\@.
\item Future work\@.
\end{itemize}

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

Thank you to the members of the Kedziora Research Lab for feedback and suggestions through out the process: Jonny Keane, Dr. Jeremy Kedziora, and Salvin Chowdhury.

Thank you to the members of the capstone committee for supervising this work: Dr. John Bukwoy, Dr. Rob Hasker, and Dr. Jeremy Kedziora.

Finally, thank you to Dr. RJ Nowling for providing the computing platform used during this work.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{final-report}
\bibliographystyle{icml2022}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{You \emph{can} have an appendix here.}

You can have as much text here as you want. The main body must be at most $8$ pages long.
For the final version, one more page can be added.
If you want, you can use an appendix like this one, even using the one-column format.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
